---
title: "정보 검색 기말고사"
author: dapin1490
date: 2022-10-31T19:01:00+09:00
categories: [지식, IT]
tags: [지식, IT, 정보 검색, 필기]
render_with_liquid: false
---

<style>
  .x-understand { color: #ccb833; }
  .understand { color: #1380da; }
  .tab { white-space: pre; }
  .underline { text-decoration: underline; }
  .cancle { text-decoration: line-through; }
  .green { color: forestgreen;}
  figure { text-align: center; }
</style>

<!--
<span class="x-understand"></span>
<span class="understand"></span>
<span class="tab"></span>
<span class="underline"></span>

[<a id="" href="">1</a>] #
[<a id="" href="" title="">2</a>] #, <a href="#" target="_blank">#</a>
<sup><a id="" href="" target="_blank" title=""></a></sup>

<figure>
  <img src="/assets/img/category-#/#">
  <figcaption>#</figcaption>
</figure>

<details>
  <summary>#</summary>
  <figure>
    <img src="/assets/img/category-#/#">
    <figcaption>#</figcaption>
  </figure>
</details>
-->

## 알림
모든 이미지의 출처는 "정보 검색" 강의 자료이다.

## 9주
주제 : 질의어-문서 계산 빠르게 하는 법  
- 코사인 점수 계산 줄이는 방법 : 문서별 계산 X, 단어별로 계산하기, 0은 곱하지 않기
  
### 코사인 점수 랭킹 효과적으로 골라내기
**검색 결과를 반환할 때는 코사인 점수가 높은 k개의 문서를 찾아줘야 한다. 전체 문서가 아주 많으면 코사인 점수를 정렬하는 것도 일이다.**  
- 더 빠른 코사인 점수 : 가중치 없는 쿼리 계산 안 하기.
- 상위 k개 문서 효과적으로 고르기 : 힙 사용. 노드 1백만 개, 결과 문서 100개라고 치면 전체 정렬과 비교해서 연산 횟수가 60만:20만 정도로 차이남.
- 대충 고르기 : 덜 정확하지만(코사인 점수도 정확한 거 아니니까 상관 없음) 빠름. 먼저 검색 대상 그룹을 한정하고(전체 문서 수보다는 적지만 필요한 결과 수보다는 많게) 그 안에서만 고름. 추려낸 문서는 실제 상위 k개 문서를 포함하지 않을 수 있지만 가능성은 높다고 봄. 아래는 검색 대상 그룹을 한정하는 방법들
  1. Index elimination : 질의로 들어온 단어 중 최소 하나 이상이 포함된 문서만 추리기. 아래는 추가로 추리는 방법.
    - idf가 높은 단어만 검색하기 : idf가 낮은 건 별로 안 중요한 단어다. 흔한 단어의 idf는 작아서 전체 점수에 큰 영향을 못 준다.
    - 전체 질의에서 몇 개 이상이 포함된 문서만 골라 검색하기
  2. Champion lists : tf 기준 상위 r(> k)개의 문서. 인덱싱할 때 미리 만들어둠.
  3. Static quality scores : 문서의 권위를 수치화(문서의 권위: 얼마나 믿을만한 문서인지). 질의와 무관하니 static함. 챔피언 리스트와 결합 가능
  4. Impact ordering : 모든 문서를 w<sub>t,d</sub> 순으로 정렬 (w<sub>t,d</sub>는 tf \* idf). 이 정렬로 상위 k개 문서 고르는 방법 2가지
    - 조기종료 : 검색 도중 정해진 개수의 문서를 다 봤거나, w<sub>t,d</sub>가 미리 정해둔 값보다 떨어지면 검색하지 말자
    - idf순 단어 : idf 값이 미리 정해진 값보다 큰 것만 계산하자. Index elimination에도 적용 가능
  5. Cluster pruning(가지치기)
    - 전처리 : 전체 문서 중 √n개 추출해(랜덤 샘플링) 리더 임명 후 유사도 계산해 비슷한 것끼리 군집화
    - 쿼리 전처리 : 질의와 리더들의 유사도 계산 후 유사도가 높은 클러스터만 검색
    - General variants : 전처리 단계에서 각 팔로워가 여러 리더를 따르도록 처리, 질의를 처리할 때 여러 개의 클러스터에 대해 검색하도록 함 → 결과가 정확해질 가능성이 높아지지만 계산량 많아짐
- Tiered indexes : 포스팅 리스트를 둘 이상의 티어로 나누어 생성, 높은 티어부터 검색. 티어는 인덱스 생성 시 분류함.
- Query parsers : 사용자가 free text로 검색하면 파서가 1개 이상의 쿼리 생성해 검색, 다 해봐도 안 되면 벡터 검색
- Aggregate scores : 여러 방식으로 검색한 결과의 score 합치기. 전문가가 튜닝하거나 기계학습 이용.
- Vector Space Model vs Boolean Model : 벡터 모델의 인덱스는 불린 모델 검색에도 사용 가능, 역은 불가능


## 10주
주제 : 정보 검색 시스템 평가하기  
### 검색 엔진 평가
- 검색 엔진 평가 기준  
  대부분 요소는 양적 평가 가능
  - 사용자 만족도 측정 : 일반적으로 만족도 평가는 어려우니 검색 결과와 원하던 결과의 관련성을 평가에 이용함
    - 웹 : 사용자가 원하는 정보를 찾으면, 사용자는 다음에 또 쓴다. → 재사용 사용자의 비율로 평가 가능
    - 쇼핑 사이트 : 쇼핑몰 운영자를 만족시켜야 한다(돈 주니까). 구매자가 원하는 물건을 잘 찾을 수 있는지, 구매하기까지 걸리는 시간, 검색자와 구매자의 비율(검색만 하고 가는 사람)
    - 기업 내부망 검색 엔진 : 회사/정부/학교 등에서 사용, 얼마나 사용자의 생산성을 높일 수 있는지
  - 연관성 판단 : "원하는 정보"가 제대로 나왔는지.
- 정보 검색 시스템 평가를 위한 테스트셋  
  문서 집합과 질의/해당 문서 조합인 테스트셋 필요. 문서의 양에 따라 모든 문서에 정답이 표기되지 않을 수 있음. 여러 개의 테스트 질의로 평균냄.
  - Cranfield Collection
  - TREC (Text REtrieval Conference)
  - GOV2
  - NTCIR (NII Test Collections for IR systems)
  - REUTERS-21578 and Reuters-RCV1
  - 20 Newsgroups
- 검색 결과에 순서가 없는 모델의 평가 방법
  - <span class="cancle">정확도</span> : 전체 문서 중 질의와의 관련성을 맞힌 것의 비율. 수많은 문서 중 관련 있는 몇 개만을 검색하기 때문에 '관련 없음 - 관련 없음' 정답이 너무 많아서 정확도가 너무 높게 나옴. 심지어 아무 검색 결과도 안 나와도 정확도는 99.9% 가능.  
    사람들은 쓰레기가 좀 섞여도 뭔가를 찾아주긴 하는 검색 엔진을 원한다.
  - || **관련 있음** | **관련 없음**
    |-|-|-|
    **검색 됨** | true positive(tp) | false positive(fp)
    **검색 안 됨** | false negative(fn) | true negative(tn)
  - 정밀도 : `검색된 관련 있는 문서 / 검색 결과`, P = tp / (tp + fp)
  - 재현율 : `검색된 관련 있는 문서 / 문서 집합 중 관련 있는 문서`, R = tp / (tp + fn)
  - 정밀도 vs 재현율 : 검색 결과에서 원하는 답을 빨리 찾고 싶다면(일반인) 정밀도, 딱 원하는 문서가 있기만 하면 된다면(전문가) 재현율이 높아야 한다.
- 검색 결과에 순서가 있는 모델의 평가 방법
  1. Interpolated(보간된) Precision : 특정 재현율에서, 재현율을 증가시켰을 때 얻을 수 있는 정밀도의 최댓값을 취함  

    <figure>
      <img src="/assets/img/category-it/221119-1-interpolated-precision.jpg">
      <figcaption>[이미지 1] Interpolated Precision</figcaption>
    </figure>

  2. 11-point Interpolated Average Precision
    1. 재현율을 0부터 1까지 0.1 단위로 끊어 11개의 점 준비
    2. 그 11개의 점에 대한 최대 정밀도(interpolated P) 구함
    3. 위 과정을 각 질의어마다 수행함
    4. 그 산술평균을 구함
    - 예를 들어 50개의 질의가 있다면 50번 * 11개 정밀도 구하고 평균
  3. Mean Average Precision (MAP)  
    전체 질의 집합 Q에 속하는 어떤 질의 q와, q와 관련된 문서 집합(d1, d2, ..., dm)에 대해  
    각 문서가 검색 결과에 나오는 경우의 정밀도를 계산(d1이 나올 때 정밀도 ~ dm이 나올 때 정밀도)  
    그 모든 정밀도의 합을 구하고 관련 문서 집합의 크기(m)로 나눔  
    이 과정을 Q에 속하는 모든 질의에 대해 반복하고 Q의 크기로 나눔  
    → 평균의 평균을 구하는 정밀도  

    <figure>
      <img src="/assets/img/category-it/221119-2-mean-average-precision.jpg">
      <figcaption>[이미지 2] Mean Average Precision 식</figcaption>
    </figure>

    이 방법은 여러 개의 시스템을 서로 비교할 때 주로 사용한다. 하나의 시스템을 평가하는 데에는 별로 적절하지 않다.

  4. R-precision : 주어진 질의와 관련된 문서가 몇 개인지 미리 알고 있어야 사용 가능한 방법  
    주어진 질의와 관련된 문서의 개수가 검색 결과의 최대 개수가 된다. 예를 들어 10개의 문서가 관련되었다고 한다면 검색 결과를 10개로 제한.  
    그중 실제로 관련 있는 문서의 비율을 본다(정밀도를 계산한다)  
    이 정의에 의해, R 정밀도는 재현율과 같다.  
  5. Precision at k  
    벡터 모델에서 k개를 검색했을 때 관련 있는 문서가 몇 개 있는지 보는 방법. R 정밀도와 다른 점은 검색 결과의 수인 k를 사용자 마음대로 정한다는 것.  
    질의에 따라 관련 있는 문서의 수가 다르지만 그것을 무시하므로 그다지 적절한 평가 방법이 아니다.  
- 다른 평가 방법
  1. Interjudge Agreement(상호 평가) : 사람이 검색 결과의 적절성을 직접 평가(안 할 수는 없음). 하지만 사람의 평가는 개인차가 크니 두 사람씩 조합해 평균값을 지표로 사용한다.  
    일정한 테스트셋을 만들어서 매번 재사용한다(시간과 노동력 절약)
    - Kappa Measure : 두 평가자 사이의 의견이 일치한 정도를 숫자로 표시한다.  

      <details>
        <summary>Kappa Measure 강의자료 보기</summary>
        <figure>
          <img src="../assets/img/category-it/221119-3-kappa-measure-1.jpg">
          <figcaption>Kappa Measure 강의자료 1</figcaption>
        </figure>
        <figure>
          <img src="../assets/img/category-it/221119-4-kappa-measure-2.jpg">
          <figcaption>Kappa Measure 강의자료 2</figcaption>
        </figure>
        <figure>
          <img src="../assets/img/category-it/221119-5-kappa-measure-3.jpg">
          <figcaption>Kappa Measure 강의자료 3</figcaption>
        </figure>
      </details>
  2. Evaluation of large search engines(대형 검색 엔진 평가) : k정밀도, 관련도와 상관 없는 평가 방법 사용 가능
    - Non-relevance-based measures 관련도와 상관 없는 평가 방법
      - 사용자의 클릭 정보 이용 : 클릭 1번은 별 의미가 없지만 모이면 의미가 생김
      - A/B 테스팅 : 버전 업그레이드할 때 두 버전의 성능을 비교하는 데 사용. 대부분의 검색은 기존 시스템으로 보내고 일부만을 새 시스템으로 보내서 사용자의 클릭 정보로 두 시스템의 성능을 비교

### 검색 결과 요약하기
순수한 검색 결과는 해당 문서로 가는 URL뿐. 사용자가 검색 결과를 알아보기 쉽게 각 URL에 따른 문서를 요약해서 스니펫으로 같이 보여줌.  
스니펫은 &lt;meta&gt; 태그의 내용 등 웹문서의 주요 내용을 이용해 구성함. 질의와 무관하게 고정 스니펫을 제공하거나, 질의에 따라 맞춤 스니펫을 제공할 수 있다.
- 고정 스니펫 : 미리 발췌한 문서의 부분집합.
  - 아주 간단한 방법 : 웹문서 앞 부분 몇 문장 발췌, 인덱싱할 때 만듦.
  - 좀 더 세련된 방법 : 문서의 내용과 관련된 문장 고르기. 각 단어별 가중치를 구하고 문장마다 그 값의 합을 이용해 중요도로 간주.
  - 더 더 세련된 방법 : 문서 내용 자체를 이해하고 요약을 재구성. 이론적인 방법이며 실제로는 안(못)씀.
- 동적 스니펫 : 검색 단어가 포함된 문장의 앞뒤 맥락 보여주기, 문서의 일부를 미리 갖고 있다가(cache) 그중에서 검색어가 등장하는 문장만 스니펫에 노출
  - 문제 : 스니펫을 보여줄 공간이 적고, 요약 문장을 잘 골라야 함
  - 해결 : 문서 첫 페이지 통으로 보여주기


## 11주
주제 : 관련성 피드백과 쿼리 확장  
- high recall을 위한 방법
  - **Automatic Refinement of Queries(자동 질의 확장)** : 같은 개념을 지칭하는 서로 다른 단어를 검색하면 찾아주기 힘들다 → 질의 확장
  - Global methods(static, 사전을 이용해 비슷한 단어로 질의 확장) : Thesaurus(thㅣ쏘ㄹ로스, 유의어/반의어 사전)와 WordNet(상위어 하위어 관계 사전)
    - Query Expansion(사전이 있을 때) : 쿼리에 단어 추가(수동), 유의어 사전 이용한 쿼리 확장(새로 추가된 단어에 작은 가중치 가능. 보통 재현율 ↑, 정밀도 ↓). 사전 생성부터가 수작업이라 많이 힘든 게 단점
    - Automatic Thesaurus Generation(사전이 없을 때) : 자동 유의어 사전 만들기 - 두 단어가 문서에 함께 등장하는(co-occurrence) 통계를 이용 / 문법적으로 비슷한 위치에 나왔을 때 비슷한 단어로 간주. 어차피 결과 내 재검색과 비슷한 거라 검색 결과가 크게 달라지지 않을 수 있음.
  - Local methods (dynamic) : 사용자가 검색 결과가 잘 나왔는지 피드백 후 재검색
    - Relevance Feedback(이 피드백을 받지 않는 시스템을 'ad hoc 애드 혹'이라고 함)  
      1. 사용자가 짧고 간단한 질의 생성
      2. 시스템이 해당 질의에 대한 결과 반환
      3. 사용자가 결과의 관련 여부를 평가
      4. 시스템이 피드백 결과를 반영
      5. 충분할 때까지 반복
    1. Key Concepts: Centroid - Theoretically Optimal Query : 벡터의 무게중심 이용, 어떤 질의와 모든 문서에 대한 관련성 피드백을 다 받았다는 전제로, 관련 있는 문서의 센트로이드와 관련 없는 문서의 센트로이드를 빼 최적의 질의 생성. 현실적으로 어려우므로 '가 최적 질의'를 만들기 위해 다른 방법 사용  
        - Centroid : 각 문서별 벡터의 같은 위치 요소를 다 더하고 그 개수로 나눔. 그걸 벡터의 모든 요소에 대해 함. 그 결과가 Centroid 값. 쉽게 말해 벡터 각 요소의 평균으로 이루어진 벡터.
    2. Relevance feedback - Rocchio Algorithm : 일부 결과에 대해 받은 피드백을 반영해 개선된 질의 생성
        - 전체 중 사용자가 피드백한 문서에 한해 그들의 센트로이드 값을 서로 빼고 초기 질의의 값을 더함. 가중치 있음. 보통 사용자에게 '관련 있음' 피드백만 받기 때문에 '관련 없음' 피드백 가중치는 0으로 준다.
    3. Pseudo relevance feedback : 쿼리 결과에 대해 사용자가 많이 클릭한 문서를 '관련 있음'으로 간주. 평균적으로 효과가 있는 편.
    4. Indirect relevance feedback : 간접적/묵시적 관련성 피드백. 3번과 달리 질의를 수정하지 않고 사용자의 클릭 데이터를 이용해 더 관련 있어 보이는 문서를 노출함. 클릭 스트림 마이닝이라고 부르며 명시적 피드백보다 덜 믿을만해도 유용함
    5. Evaluation of Relevance Feedback(관련성 피드백 평가하기) : 보통 두 번째 이후 수정은 큰 의미 없으니 한 번만 수정해도 됨.
       1. 시험 보고 똑같은 문제로 재시험보기(개선이 당연하니 부적절)
       2. 시험 보고 맞은 문제 빼고 재시험보기 : 여러 시스템의 비교에는 쓸만하지만 피드백 유무에 따른 성능 비교에는 부적절
       3. 시험용 문서집합과 재시험용 문서집합 나눠서 시험하기
- 확장 쿼리의 단점
  - 계산량 증가로 결과 제공 느려짐
  - 컴퓨팅 자원 많이 필요
  - 부분적 해결 방법은 있지만 그래도 원래 쿼리보다 비효율적

## 12주
주제 : 웹 검색 기초  

- Web Search Characteristics : 웹 크롤러가 웹을 돌아다녀 문서를 크롤링하고, 사용자가 무언가 검색할 때에는 크롤러가 만든 인덱스와 광고용 인덱스 둘 다 검색함.
  - 웹문서 특징
    - 생성 : 중앙집중화되지 않음. 분산되고 민주화된 생성. 생성자의 배경이 다양
    - 컨텐츠 : 최신, 구식, 진짜, 가짜가 섞여 있고 언어도 다양. static한 문서도 있고 동적으로 생성되는 문서도 있음. 요청할 때 즉석에서 만들어주는 문서는 동적 생성 문서이므로 실제로 존재하는 문서가 아님.
    - 구조 : 없는 것, 반구조화된 것, 구조화된 것 존재
    - 스케일 : 점점 증가.
  - 웹 정보 겁색 : 검색 엔진 매우 중요
    - 키워드 기반 검색 : 불린, 벡터 모델
    - Taxonomy-based search : 크롤러가 가져온 문서를 사람이 카테고리별로 분류함. 노동력이 많이 필요하고 주관의 영향을 받음
    - 그 외 : 스팸과 싸우는 테크닉 필요, 문서가 있는 사이트의 권위 구분
  - Web Search User Experience
    - 정통 정보 검색 : 보통 전문가가 쓰고, 잘 검색할 줄 알고 있음
    - 웹 검색 : 비전문가가 사용, 검색 대충 함
  - User Query Needs : 검색 목적
    - 정보 필요, 특정 목적지가 있음, 상거래/이미지 다운로드 등 트랜잭션, 특정 부류가 모인 허브 찾고 싶음
- 웹 그래프 : 링크는 랜덤하게 분포하지 않음. 웹페이지 링크 분포는 푸아송비를 따르지 않음. 들어가는 링크 개수에 따라 그런 문서가 존재하는 비율은 링크 개수의 가중치 제곱분의 일. 
- Advertising as Economic Model
  - 전화번호부 : 돈을 내면 글자를 크고 굵게 써줌(광고), 사전순 정렬과 의미 이용한 꼼수 가능(`aaaaaaCheapestPhoneCompanyaaaaa`)
  - 초기 광고 : 브랜드 자체를 배너 광고, 배너 노출 횟수에 따른 요금
  - 요즘 광고 : 목표는 실제 트랜잭션이 일어나게 하는 것, 클릭 횟수에 따른 요금
  - 키워드 경매 광고 사이트 GOTO
- Spam
  - SEO : 서치 엔진 최적화, 광고 넣지 않고 알고리즘이 내 글 위에 올려주게 하기. 회사에 돈 주고 맡길 수 있고, 꼼수 있고, 어둠의 루트(모름) 있음
  - 키워드 채우기 : 게시글 바탕과 똑같은 색으로 키워드를 잔뜩 써놓는다. 사용자에게는 안 보이지만 문서의 tf-idf 값을 높인다. idf는 못 건드리고 tf에 영향을 줌.
  - 메타 태그 채우기
  - Cloaking : 웹크롤러와 사용자에게 서로 다른 것을 보여줌.
  - Doorway Page : 링크가 변경된 문서에 대해 크롤러에게는 문서가 있는 척 하고 사용자는 새 문서로 리다이렉트시킴.
  - 클릭 스팸 : 딱 정의된 건 아님. 예 - 오토마우스로 경쟁사의 광고를 잔뜩 클릭해서 광고비 소진시키고 자사 광고 넣기
- Index Size Estimation : 서로 다른 검색 엔진의 인덱스 사이즈 비교하기. 몬테카를로 원주율 참고.
  - 랜덤하고 균일하게 분포하는 링크를 생성해야 하지만 그게 잘 안 됨. 양쪽 엔진에서 랜덤 추출하는 것으로 타협.
  - 쿼리 로그 이용하기 : 기존 사용자의 편향에 영향 받음
  - IP 주소 랜덤 생성하기
  - 사전에서 랜덤 단어 골라 랜덤 쿼리 만들기
- Duplicate Documents


12주 46페이지부터 이어서

## 13주


## 14주


## 참고
&#91;1&#93; 마크다운 - 표(테이블) 만들기, <a href="https://inasie.github.io/it%EC%9D%BC%EB%B0%98/%EB%A7%88%ED%81%AC%EB%8B%A4%EC%9A%B4-%ED%91%9C-%EB%A7%8C%EB%93%A4%EA%B8%B0/" target="_blank">https://inasie.github.io/it일반/마크다운-표-만들기/</a>
